{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 1: Entrenamiento\n",
    "\n",
    "Originalmente, se pretendía que las etapas de hiperafinación y entrenamiento se realizaran dentro del mismo *notebook*, pero rápidamente notamos que al desplegarlos, las instancias simplemente se caían debido al agotamiento de la RAM (OOM). La causa está en que la librería \"Keras Tuner\" por cada *trial* acumula más y más memoria RAM de manera absurda sin liberarla una vez finalizada la etapa de hiperafinación. Es por esto que la fase 1 se divide en dos etapas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de la GPU\n",
    "\n",
    "Por un motivo que se desconoce, cuando se utiliza el acelerador P100, es necesario limitar el crecimiento de la GPU (para más detalles, revisar [acá](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)). Otro punto es que esta celda debe venir antes de cualquier importación ya que internamente modifican la capacidad de la GPU (ver el siguiente [hilo](https://github.com/hunglc007/tensorflow-yolov4-tflite/issues/171)), y por ende, obtenemos el error `Physical devices cannot be modified after being initialized`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.config import list_physical_devices\n",
    "from tensorflow.config.experimental import set_memory_growth\n",
    "\n",
    "gpus = list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(\"error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de dependencias\n",
    "\n",
    "Esta imagen viene con librerías preinstaladas de Tensorflow y Hugging Face, por lo que únicamente deberemos instalar `dotwiz`, el cual permite transformar un diccionario a una notación de puntos (tal y como accedemos a métodos y atributos en Javascript)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install dotwiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiaremos el nivel de *logs* generados por Hugging Face ya que genera advertencias que no aplican en nuestro caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización de constantes\n",
    "\n",
    "Definiremos al objeto `CONSTANTS` que contendrá todas las constantes a utilizar organizadas de forma jerárquica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotwiz import DotWiz\n",
    "from math import log, floor\n",
    "from logging import info\n",
    "from re import compile\n",
    "\n",
    "CONSTANTS = DotWiz({\n",
    "    \"DATASETS\": {\n",
    "        \"PAN\": {\n",
    "            \"PATHS\": {\n",
    "                \"TRAINING\": {\n",
    "                    \"CONVERSATIONS\":\n",
    "                    \"inputs/pan-2012-training/pan12-sexual-predator-identification-training-corpus-2012-05-01.xml\",\n",
    "                    \"SEXUAL_PREDATORS\":\n",
    "                    \"inputs/pan-2012-training/pan12-sexual-predator-identification-training-corpus-predators-2012-05-01.txt\"\n",
    "                },\n",
    "                \"TESTING\": {\n",
    "                    \"CONVERSATIONS\":\n",
    "                    \"inputs/pan-2012-testing/pan12-sexual-predator-identification-test-corpus-2012-05-17.xml\",\n",
    "                    \"SEXUAL_PREDATORS\":\n",
    "                    \"inputs/pan-2012-testing/pan12-sexual-predator-identification-groundtruth-problem1.txt\",\n",
    "                    \"SUSPICIOUS_CONVERSATIONS\":\n",
    "                    \"inputs/pan-2012-testing/pan12-sexual-predator-identification-groundtruth-problem2.txt\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"MODELS\": {\n",
    "        \"BERT\": {\n",
    "            \"NAME\": \"bert-base-uncased\",\n",
    "            \"TOKENIZER\": {\n",
    "                \"MAX_LENGTH\": 128,\n",
    "                \"PADDING\": \"max_length\",\n",
    "                \"TRUNCATION\": True\n",
    "            },\n",
    "            \"BATCHING\": {\n",
    "                \"SIZE\": 32\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"PREPROCESSING\": {\n",
    "        \"RANDOMNESS\": {\n",
    "            \"SEED\": 400\n",
    "        },\n",
    "        \"MESSAGE_BATCHING\": {\n",
    "            \"SIZE\": 24\n",
    "        },\n",
    "        \"REGEXES\": {\n",
    "            \"XML_QUOTE_ESCAPES\": compile(r\"&(apos|quot);\")\n",
    "        },\n",
    "        \"DATASET_SPLITS\": {\n",
    "            \"TRAINING\": 0.8,\n",
    "            \"VALIDATION\": 0.2\n",
    "        }\n",
    "    },\n",
    "    \"TRAINING\": {\n",
    "        \"SCHEDULES\": {\n",
    "            \"EXPONENTIAL_DECAY\": {\n",
    "                \"DECAY_STEPS_FACTOR\": 1 / 2,\n",
    "                \"DECAY_RATE\": 0.97\n",
    "            },\n",
    "            \"ADAM_WEIGHT_DECAY\": {\n",
    "                \"BETA_1\": 0.9,\n",
    "                \"BETA_2\": 0.999,\n",
    "                \"WEIGHT_DECAY_RATE\": 0.01\n",
    "            }\n",
    "        },\n",
    "        \"EPOCHS\": 4,\n",
    "        \"WARMUP_FACTOR\": 0.1,\n",
    "        \"INITIAL_LEARNING_RATE\": 2e-5,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder recrear (hasta cierto punto) cada uno de los resultados propuestos, preestableceremos una semilla. Es posible que existan diferencias entre una ejecución y otra ya que como trabajaremos a nivel de GPU, muchas de las operaciones en Tensorflow son procesadas de manera asíncrona, y muchos de los valores que tratamos acá requieren sumar flotantes que sí se ven afectados cuando cambian sus órdenes. Si quisiéramos habilitar un determinismo completo, usaríamos la instrucción `tensorflow.config.experimental.enable_op_determinism()`, pero veríamos una degradación en el desempeño de las instrucciones en varios órdenes de magnitud. Para mayores detalles, revisar la [documentación oficial](https://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism) de Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import set_random_seed\n",
    "\n",
    "set_random_seed(CONSTANTS.PREPROCESSING.RANDOMNESS.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación del conjunto de entrenamiento y validación\n",
    "\n",
    "El conjunto de datos de **entrenamiento** PAN2012 tiene dos archivos de utilidad:\n",
    "\n",
    "1. `pan12-sexual-predator-identification-training-corpus-predators-2012-05-01.txt`: Enlista todos los identificadores de los autores que (se sabe) son depredadores sexuales separados por saltos de línea.\n",
    "1. `pan12-sexual-predator-identification-training-corpus-2012-05-01.xml`: Enlista tanto conversaciones normales como pervertidas en un formato de etiquetas. Cada conversación se encierra con `<conversation>` y cada mensaje por `<message>`.\n",
    "\n",
    "La cantidad de conversaciones normales versus pervertidas está altamente desequilibrada, por lo que se hará un tratamiento básico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import cElementTree as ET\n",
    "from xml.sax.saxutils import unescape\n",
    "from re import sub\n",
    "from pandas import Series\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "def parse_xml_to_hf_dataset(sexual_predators_path, conversations_path):\n",
    "    dataset = {\n",
    "        \"conversation_id\": [],\n",
    "        \"conversation_label\": [],\n",
    "        \"message\": [],\n",
    "    }\n",
    "\n",
    "    with open(sexual_predators_path, \"r\") as file:\n",
    "        sexual_predators = []\n",
    "\n",
    "        for sexual_predator in file.readlines():\n",
    "            sexual_predator = sexual_predator.strip()\n",
    "            sexual_predators.append(sexual_predator)\n",
    "\n",
    "    for event, element in ET.iterparse(conversations_path,\n",
    "                                       events=(\"start\", \"end\")):\n",
    "        if event != \"end\" or element.tag != \"conversation\":\n",
    "            continue\n",
    "\n",
    "        conversation_id = element.get(\"id\").strip()\n",
    "        messages = element.findall(\"message\")\n",
    "        unescaped_messages = []\n",
    "\n",
    "        conversation_includes_sexual_predator = False\n",
    "\n",
    "        for index, message in enumerate(messages):\n",
    "            author = message.find(\"author\").text.strip()\n",
    "            message = message.find(\"text\")\n",
    "\n",
    "            if message is None:\n",
    "                continue\n",
    "\n",
    "            if message.text is None:\n",
    "                continue\n",
    "\n",
    "            if author in sexual_predators:\n",
    "                conversation_includes_sexual_predator = True\n",
    "\n",
    "            unescaped_message = sub(\n",
    "                CONSTANTS.PREPROCESSING.REGEXES.XML_QUOTE_ESCAPES, \"'\",\n",
    "                unescape(message.text.strip()))\n",
    "            unescaped_messages.append(unescaped_message)\n",
    "\n",
    "        if not unescaped_messages:\n",
    "            continue\n",
    "\n",
    "        for index in range(0, len(unescaped_messages),\n",
    "                           CONSTANTS.PREPROCESSING.MESSAGE_BATCHING.SIZE):\n",
    "            dataset[\"conversation_id\"].append(conversation_id)\n",
    "            dataset[\"conversation_label\"].append(\n",
    "                conversation_includes_sexual_predator)\n",
    "            dataset[\"message\"].append(\"\\n\".join(\n",
    "                unescaped_messages[index:index + CONSTANTS.PREPROCESSING.\n",
    "                                   MESSAGE_BATCHING.SIZE]))\n",
    "\n",
    "    conversation_series = Series(dataset[\"conversation_label\"])\n",
    "    normal_conversation_series = conversation_series[conversation_series ==\n",
    "                                                     False]\n",
    "    perverted_conversation_series = conversation_series[conversation_series ==\n",
    "                                                        True]\n",
    "    normal_conversation_series = normal_conversation_series.sample(\n",
    "        frac=len(perverted_conversation_series) /\n",
    "        len(normal_conversation_series),\n",
    "        random_state=CONSTANTS.PREPROCESSING.RANDOMNESS.SEED)\n",
    "\n",
    "    hf_dataset = Dataset.from_dict(dataset)\n",
    "    hf_dataset = hf_dataset.select(\n",
    "        perverted_conversation_series.index.to_list() +\n",
    "        normal_conversation_series.index.to_list())\n",
    "\n",
    "    return hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializaremos un objeto llamado `tokenizer` que permitirá dividir sentencias en subpalabras (*tokens*) para asociarlas con un identificador (*token ids*) dentro de un vocabulario. Es una instancia específica para el modelo BERT llamado `CONSTANTS.MODELS.BERT.NAME` de Hugging Face. Su salida será diccionario con tres claves:\n",
    "\n",
    "- `input_ids`: Contendrá una matriz con los identificadores de las palabras divididas.\n",
    "- `token_type_ids`: Contendrá una matriz de correspondencia de las  palabras divididas a alguna de las frases en la tupla. Como nuestro caso no es QA, esta tupla en realidad no existe, y lo único que veremos como salida será un vector de elementos `0`.\n",
    "- `attention_mask`: Contendrá una matriz que considerará o no a las palabras divididas en el procesamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONSTANTS.MODELS.BERT.NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos una función que aplique el algoritmo Subword Tokenization a través del objeto `tokenizer` en cada uno de los mensajes. El resultado deberá ser retornado en dos conjuntos: uno de tamaño  `left_side_proportion` y otro de tamaño `1 - left_side_proportion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "def hf_dataset_to_tf_dataset(hf_dataset,\n",
    "                             left_side_proportion=None,\n",
    "                             return_hf=False):\n",
    "    hf_dataset = hf_dataset.map(lambda example: tokenizer(\n",
    "        example[\"message\"],\n",
    "        max_length=CONSTANTS.MODELS.BERT.TOKENIZER.MAX_LENGTH,\n",
    "        padding=CONSTANTS.MODELS.BERT.TOKENIZER.PADDING,\n",
    "        truncation=CONSTANTS.MODELS.BERT.TOKENIZER.TRUNCATION))\n",
    "    hf_dataset = hf_dataset.shuffle(\n",
    "        seed=CONSTANTS.PREPROCESSING.RANDOMNESS.SEED)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer,\n",
    "                                            return_tensors=\"tf\")\n",
    "    to_tf_dataset_kwargs = {\n",
    "        \"columns\": [\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "        \"label_cols\": [\"conversation_label\"],\n",
    "        \"batch_size\": CONSTANTS.MODELS.BERT.BATCHING.SIZE,\n",
    "        \"collate_fn\": data_collator,\n",
    "        \"shuffle\": False\n",
    "    }\n",
    "\n",
    "    if left_side_proportion is None:\n",
    "        if return_hf:\n",
    "            return (hf_dataset.to_tf_dataset(**to_tf_dataset_kwargs),\n",
    "                    hf_dataset), (None, None)\n",
    "\n",
    "        return (hf_dataset.to_tf_dataset(**to_tf_dataset_kwargs), None), (None,\n",
    "                                                                          None)\n",
    "\n",
    "    hf_dataset = hf_dataset.train_test_split(train_size=left_side_proportion,\n",
    "                                             shuffle=None)\n",
    "    tf_left_side_dataset = hf_dataset[\"train\"].to_tf_dataset(\n",
    "        **to_tf_dataset_kwargs)\n",
    "    tf_right_side_dataset = hf_dataset[\"test\"].to_tf_dataset(\n",
    "        **to_tf_dataset_kwargs)\n",
    "\n",
    "    if return_hf:\n",
    "        return (tf_left_side_dataset,\n",
    "                hf_dataset[\"train\"]), (tf_right_side_dataset,\n",
    "                                       hf_dataset[\"test\"])\n",
    "\n",
    "    return (tf_left_side_dataset, None), (tf_right_side_dataset, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produciremos el conjunto de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = parse_xml_to_hf_dataset(\n",
    "    CONSTANTS.DATASETS.PAN.PATHS.TRAINING.SEXUAL_PREDATORS,\n",
    "    CONSTANTS.DATASETS.PAN.PATHS.TRAINING.CONVERSATIONS)\n",
    "\n",
    "(tf_training_dataset,\n",
    " hf_training_dataset), (tf_validation_dataset,\n",
    "                        hf_validation_dataset) = hf_dataset_to_tf_dataset(\n",
    "                            hf_dataset,\n",
    "                            left_side_proportion=1 -\n",
    "                            CONSTANTS.PREPROCESSING.DATASET_SPLITS.VALIDATION,\n",
    "                            return_hf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del modelo\n",
    "\n",
    "Recuperaremos el modelo pre-entrenado de BERT y lo compilaremos para ajustarlo a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamWeightDecay, WarmUp, TFBertForSequenceClassification\n",
    "from tensorflow.data.experimental import cardinality\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "STEPS_PER_EPOCH = cardinality(tf_training_dataset).numpy()\n",
    "TOTAL_TRAINING_STEPS = STEPS_PER_EPOCH * CONSTANTS.TRAINING.EPOCHS\n",
    "TOTAL_WARMUP_STEPS = int(CONSTANTS.TRAINING.WARMUP_FACTOR *\n",
    "                         TOTAL_TRAINING_STEPS)\n",
    "\n",
    "learning_rate_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=CONSTANTS.TRAINING.INITIAL_LEARNING_RATE,\n",
    "    decay_steps=STEPS_PER_EPOCH *\n",
    "    CONSTANTS.TRAINING.SCHEDULES.EXPONENTIAL_DECAY.DECAY_STEPS_FACTOR,\n",
    "    decay_rate=CONSTANTS.TRAINING.SCHEDULES.EXPONENTIAL_DECAY.DECAY_RATE)\n",
    "warmup_schedule = WarmUp(\n",
    "    initial_learning_rate=CONSTANTS.TRAINING.INITIAL_LEARNING_RATE,\n",
    "    decay_schedule_fn=learning_rate_schedule,\n",
    "    warmup_steps=TOTAL_WARMUP_STEPS)\n",
    "optimizer = AdamWeightDecay(\n",
    "    learning_rate=warmup_schedule,\n",
    "    beta_1=CONSTANTS.TRAINING.SCHEDULES.ADAM_WEIGHT_DECAY.BETA_1,\n",
    "    beta_2=CONSTANTS.TRAINING.SCHEDULES.ADAM_WEIGHT_DECAY.BETA_2,\n",
    "    weight_decay_rate=CONSTANTS.TRAINING.SCHEDULES.ADAM_WEIGHT_DECAY.\n",
    "    WEIGHT_DECAY_RATE)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    CONSTANTS.MODELS.BERT.NAME, num_labels=2)\n",
    "model.compile(optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo hiperafinado\n",
    "\n",
    "Todos los modelos serán entrenados bajo la misma cantidad de *epochs* para efectos de comparación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(tf_training_dataset,\n",
    "                    validation_data=tf_validation_dataset,\n",
    "                    epochs=CONSTANTS.TRAINING.EPOCHS,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del conjunto de pruebas\n",
    "\n",
    "En cuanto al directorio no existen muchas diferencias. Los archivos que nos interesan son los siguientes:\n",
    "\n",
    "1. `pan12-sexual-predator-identification-groundtruth-problem1.txt`: Enlista todos los identificadores de los autores que (se sabe) son depredadores sexuales separados por saltos de línea.\n",
    "1. `pan12-sexual-predator-identification-test-corpus-2012-05-17.xml`: Enlista tanto conversaciones normales como pervertidas en un formato de etiquetas. Cada conversación se encierra con `<conversation>` y cada mensaje por `<message>`.\n",
    "\n",
    "Al igual que para el caso del entrenamiento, la cantidad de conversaciones normales versus pervertidas está altamente desequilibrada, por lo que se hará un tratamiento básico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = parse_xml_to_hf_dataset(\n",
    "    CONSTANTS.DATASETS.PAN.PATHS.TESTING.SEXUAL_PREDATORS,\n",
    "    CONSTANTS.DATASETS.PAN.PATHS.TESTING.CONVERSATIONS)\n",
    "(tf_testing_dataset,\n",
    " hf_testing_dataset), (_,\n",
    "                       _) = hf_dataset_to_tf_dataset(hf_dataset,\n",
    "                                                     left_side_proportion=None,\n",
    "                                                     return_hf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de métricas en base al conjunto de pruebas\n",
    "\n",
    "Dado que los conjuntos que trabajamos son instancias de la clase padre `tensorflow.data.Dataset`, no podemos acceder directamente a las etiquetas en un formato *slicing* (o similar), por lo que deberemos recorrer *batch* por *batch* y concatenar los resultados en un vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, concatenate\n",
    "\n",
    "\n",
    "def get_tf_dataset_labels(tf_dataset):\n",
    "    concatenated_labels = array([], dtype=\"int32\")\n",
    "\n",
    "    for _, labels in tf_dataset:\n",
    "        concatenated_labels = concatenate((concatenated_labels, labels))\n",
    "\n",
    "    return concatenated_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para medir la capacidad de generalización del modelo recién entrenado, definiremos la función `mesaure_model_metrics`, la cual retornará cinco valores (en orden):\n",
    "\n",
    "1. Exactitud o *binary accuracy*.\n",
    "1. Pérdida o *binary crossentropy*.\n",
    "1. Recuperación o *recall*.\n",
    "1. Precisión o *precision*.\n",
    "1. Valor-F o *F-score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "\n",
    "def measure_model_metrics(true_labels, predicted_labels, from_logits=False):\n",
    "    binary_accuracy = BinaryAccuracy()\n",
    "    binary_accuracy.update_state(true_labels, predicted_labels)\n",
    "\n",
    "    binary_crossentropy = BinaryCrossentropy(from_logits=from_logits)(\n",
    "        true_labels, predicted_labels)\n",
    "\n",
    "    recall = Recall()\n",
    "    recall.update_state(true_labels, predicted_labels)\n",
    "\n",
    "    precision = Precision()\n",
    "    precision.update_state(true_labels, predicted_labels)\n",
    "\n",
    "    f_score = 2 * (precision.result().numpy() * recall.result().numpy()) / (\n",
    "        precision.result().numpy() + recall.result().numpy())\n",
    "\n",
    "    return binary_accuracy.result().numpy(), binary_crossentropy.numpy(\n",
    "    ), recall.result().numpy(), precision.result().numpy(), f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, obtendremos las métricas de desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from tensorflow import sigmoid\n",
    "\n",
    "outputs = model.predict(tf_testing_dataset, verbose=1)\n",
    "true_labels = get_tf_dataset_labels(tf_testing_dataset)\n",
    "predicted_labels = sigmoid(outputs.logits).numpy()[:,1].flatten()\n",
    "accuracy, loss, recall, precision, f_score = measure_model_metrics(\n",
    "    true_labels, predicted_labels, from_logits=False)\n",
    "\n",
    "print(f\"\\n- accuracy: {accuracy}\")\n",
    "print(f\"- loss: {loss}\")\n",
    "print(f\"- recall: {recall}\")\n",
    "print(f\"- precision: {precision}\")\n",
    "print(f\"- f-score: {f_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar resultados\n",
    "\n",
    "Persistiremos el modelo y todos los conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data.experimental import save\n",
    "\n",
    "model_name = \"{batch_size}BS_{message_batching_size}MBS_{learning_rate}LR_{epochs}E\".format(\n",
    "    batch_size=CONSTANTS.MODELS.BERT.BATCHING.SIZE,\n",
    "    message_batching_size=CONSTANTS.PREPROCESSING.MESSAGE_BATCHING.SIZE,\n",
    "    learning_rate=CONSTANTS.TRAINING.INITIAL_LEARNING_RATE,\n",
    "    epochs=CONSTANTS.TRAINING.EPOCHS)\n",
    "\n",
    "model.save_pretrained(f\"outputs/phase_1/models/{model_name}\")\n",
    "\n",
    "save(\n",
    "    tf_training_dataset,\n",
    "    f\"outputs/phase_1/datasets/without_attentions/training/tensorflow/{model_name}\"\n",
    ")\n",
    "save(\n",
    "    tf_validation_dataset,\n",
    "    f\"outputs/phase_1/datasets/without_attentions/validation/tensorflow/{model_name}\"\n",
    ")\n",
    "save(\n",
    "    tf_testing_dataset,\n",
    "    f\"outputs/phase_1/datasets/without_attentions/testing/tensorflow/{model_name}\"\n",
    ")\n",
    "\n",
    "hf_training_dataset.save_to_disk(\n",
    "    f\"outputs/phase_1/datasets/without_attentions/training/huggingface/{model_name}\"\n",
    ")\n",
    "hf_validation_dataset.save_to_disk(\n",
    "    f\"outputs/phase_1/datasets/without_attentions/training/huggingface/{model_name}\"\n",
    ")\n",
    "hf_testing_dataset.save_to_disk(\n",
    "    f\"outputs/phase_1/datasets/without_attentions/training/huggingface/{model_name}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
