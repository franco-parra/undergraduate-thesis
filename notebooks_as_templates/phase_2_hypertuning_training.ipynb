{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 2: Hiperafinación y entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de la GPU\n",
    "\n",
    "Por un motivo que se desconoce, cuando se utiliza el acelerador P100, es necesario limitar el crecimiento de la GPU (para más detalles, revisar [acá](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)). Otro punto es que esta celda debe venir antes de cualquier importación ya que internamente modifican la capacidad de la GPU (ver el siguiente [hilo](https://github.com/hunglc007/tensorflow-yolov4-tflite/issues/171)), y por ende, obtenemos el error `Physical devices cannot be modified after being initialized`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.config import list_physical_devices\n",
    "from tensorflow.config.experimental import set_memory_growth\n",
    "\n",
    "gpus = list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(\"error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de dependencias\n",
    "\n",
    "Esta imagen viene con librerías preinstaladas de Tensorflow y Hugging Face, por lo que únicamente deberemos instalar `dotwiz`, el cual permite transformar un diccionario a una notación de puntos (tal y como accedemos a métodos y atributos en Javascript). Como haremos modificaciones a los conjuntos de datos, es necesario copiarlos porque están contenidos en el directorio `/kaggle/working` y es de lectura únicamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install dotwiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiaremos el nivel de *logs* generados por Hugging Face ya que genera advertencias que no aplican en nuestro caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización de constantes\n",
    "\n",
    "Definiremos al objeto `CONSTANTS` que contendrá todas las constantes a utilizar organizadas de forma jerárquica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotwiz import DotWiz\n",
    "\n",
    "CONSTANTS = DotWiz({\n",
    "    \"DATASETS\": {\n",
    "        \"32BS_24MBS_5e-05LR_4E\": {\n",
    "            \"TRAINING\":\n",
    "            \"outputs/phase_1/datasets/with_attentions/training/huggingface/32BS_24MBS_5e-05LR_4E\",\n",
    "            \"TESTING\":\n",
    "            \"outputs/phase_1/datasets/with_attentions/testing/huggingface/32BS_24MBS_5e-05LR_4E\",\n",
    "        },\n",
    "        \"32BS_24MBS_2e-05LR_3E\": {\n",
    "            \"TRAINING\":\n",
    "            \"outputs/phase_1/datasets/with_attentions/training/huggingface/32BS_24MBS_2e-05LR_3E\",\n",
    "            \"TESTING\":\n",
    "            \"outputs/phase_1/datasets/with_attentions/testing/huggingface/32BS_24MBS_2e-05LR_3E\",\n",
    "        }\n",
    "    },\n",
    "    \"MODELS\": {\n",
    "        \"BERT\": {\n",
    "            \"NAME\": \"bert-base-uncased\",\n",
    "            \"TOKENIZER\": {\n",
    "                \"MAX_LENGTH\": 128\n",
    "            },\n",
    "            \"BATCHING\": {\n",
    "                \"SIZE\": 32\n",
    "            },\n",
    "            \"POOLER_OUTPUT_LENGTH\": 768\n",
    "        },\n",
    "    },\n",
    "    \"PREPROCESSING\": {\n",
    "        \"RANDOMNESS\": {\n",
    "            \"SEED\": 400\n",
    "        },\n",
    "        \"DATASET_SPLITS\": {\n",
    "            \"TRAINING\": 0.8,\n",
    "            \"VALIDATION\": 0.2,\n",
    "        },\n",
    "        \"HYPERDATASET_SPLITS\": {\n",
    "            \"TRAINING\": 0.8,\n",
    "            \"VALIDATION\": 0.2,\n",
    "        },\n",
    "        \"HYPERDATASET_FRACTION\": 0.4\n",
    "    },\n",
    "    \"HYPERTUNING\": {\n",
    "        \"OPTIMIZERS\": {\n",
    "            \"SGD\": {\n",
    "                \"LEARNING_RATE\": {\n",
    "                    \"MIN_VALUE\": 1e-2,\n",
    "                    \"MAX_VALUE\": 1e-1,\n",
    "                    \"SAMPLING\": \"log\"\n",
    "                },\n",
    "                \"MOMENTUM\": {\n",
    "                    \"MIN_VALUE\": 1e-5,\n",
    "                    \"MAX_VALUE\": 5e-5,\n",
    "                    \"SAMPLING\": \"log\"\n",
    "                }\n",
    "            },\n",
    "            \"RMSPROP\": {\n",
    "                \"LEARNING_RATE\": {\n",
    "                    \"MIN_VALUE\": 1e-5,\n",
    "                    \"MAX_VALUE\": 5e-5,\n",
    "                    \"SAMPLING\": \"log\"\n",
    "                },\n",
    "                \"RHO\": {\n",
    "                    \"MIN_VALUE\": 1e-5,\n",
    "                    \"MAX_VALUE\": 1e-1,\n",
    "                    \"SAMPLING\": \"log\"\n",
    "                },\n",
    "                \"MOMENTUM\": {\n",
    "                    \"MIN_VALUE\": 1e-5,\n",
    "                    \"MAX_VALUE\": 5e-5,\n",
    "                    \"SAMPLING\": \"log\"\n",
    "                },\n",
    "                \"WEIGHT_DECAY\": {\n",
    "                    \"MIN_VALUE\": 1e-5,\n",
    "                    \"MAX_VALUE\": 1e-1,\n",
    "                    \"SAMPLING\": \"log\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"LAYERS\": {\n",
    "            \"LSTM\": {\n",
    "                \"UNITS\": {\n",
    "                    \"MIN_VALUE\": 32,\n",
    "                    \"MAX_VALUE\": 128,\n",
    "                    \"STEP\": 32\n",
    "                }\n",
    "            },\n",
    "            \"DENSE\": {\n",
    "                \"UNITS\": {\n",
    "                    \"MIN_VALUE\": 768,\n",
    "                    \"MAX_VALUE\": 1024,\n",
    "                    \"STEP\": 32\n",
    "                },\n",
    "            },\n",
    "            \"DROPOUT\": {\n",
    "                \"RATE\": {\n",
    "                    \"MIN_VALUE\": 0.1,\n",
    "                    \"MAX_VALUE\": 0.6,\n",
    "                    \"STEP\": 0.1,\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"ALGORITHMS\": {\n",
    "            \"RANDOM_SEARCH\": {\n",
    "                \"OBJECTIVE\": \"val_loss\",\n",
    "                \"MAX_TRIALS\": 50,\n",
    "                \"EPOCHS_PER_TRIAL\": 5\n",
    "            }\n",
    "        },\n",
    "        \"PATHS\": {\n",
    "            \"ROOT\": \"tmp/hypertuning\",\n",
    "            \"HYPERBAND\": {\n",
    "                \"SGD\": \"hyperband/sgd\",\n",
    "                \"RMSPROP\": \"hyperband/rmsprop\"\n",
    "            },\n",
    "            \"RANDOM_SEARCH\": {\n",
    "                \"SGD\": \"random_search/sgd\",\n",
    "                \"RMSPROP\": \"random_search/rmsprop\"\n",
    "            },\n",
    "        },\n",
    "        \"TRIALS_PER_EXECUTION\": 5,\n",
    "    },\n",
    "    \"TRAINING\": {\n",
    "        \"CALLBACKS\": {\n",
    "            \"MODEL_CHECKPOINT\": {\n",
    "                \"MONITOR\": \"val_loss\",\n",
    "                \"FILEPATH\": {\n",
    "                    \"32BS_24MBS_5e-05LR_4E\":\n",
    "                    \"outputs/phase_2/models/32BS_24MBS_5e-05LR_4E/epoch-{epoch:02d}\",\n",
    "                    \"32BS_24MBS_2e-05LR_3E\":\n",
    "                    \"outputs/phase_2/models/32BS_24MBS_2e-05LR_3E/epoch-{epoch:02d}\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"EPOCHS\": 7\n",
    "    },\n",
    "    \"INPUTS\": {\n",
    "        \"FOCUS_MODEL\": \"32BS_24MBS_5e-05LR_4E\"\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder recrear (hasta cierto punto) cada uno de los resultados propuestos, preestableceremos una semilla. Es posible que existan diferencias entre una ejecución y otra ya que como trabajaremos a nivel de GPU, muchas de las operaciones en Tensorflow son procesadas de manera asíncrona, y muchos de los valores que tratamos acá requieren sumar flotantes que sí se ven afectados cuando cambian sus órdenes. Si quisiéramos habilitar un determinismo completo, usaríamos la instrucción `tensorflow.config.experimental.enable_op_determinism()`, pero veríamos una degradación en el desempeño de las instrucciones en varios órdenes de magnitud. Para mayores detalles, revisar la [documentación oficial](https://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism) de Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import set_random_seed\n",
    "\n",
    "set_random_seed(CONSTANTS.PREPROCESSING.RANDOMNESS.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación del conjunto de entrenamiento y validación\n",
    "\n",
    "El conjunto de datos de **entrenamiento** PAN2012 tiene dos archivos de utilidad:\n",
    "\n",
    "1. `pan12-sexual-predator-identification-training-corpus-predators-2012-05-01.txt`: Enlista todos los identificadores de los autores que (se sabe) son depredadores sexuales separados por saltos de línea.\n",
    "1. `pan12-sexual-predator-identification-training-corpus-2012-05-01.xml`: Enlista tanto conversaciones normales como pervertidas en un formato de etiquetas. Cada conversación se encierra con `<conversation>` y cada mensaje por `<message>`.\n",
    "\n",
    "En cuanto al directorio de pruebas, no existen muchas diferencias. Los archivos que nos interesan son los siguientes:\n",
    "\n",
    "1. `pan12-sexual-predator-identification-groundtruth-problem1.txt`: Enlista todos los identificadores de los autores que (se sabe) son depredadores sexuales separados por saltos de línea.\n",
    "1. `pan12-sexual-predator-identification-test-corpus-2012-05-17.xml`: Enlista tanto conversaciones normales como pervertidas en un formato de etiquetas. Cada conversación se encierra con `<conversation>` y cada mensaje por `<message>`.\n",
    "\n",
    "La cantidad de conversaciones normales versus pervertidas está altamente desequilibrada, por lo que se hará un tratamiento básico. Los mensajes ya vienen agrupados desde la fase 1 y se cuentan con sus respectivos *embeddings*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, DatasetDict\n",
    "from pandas import Series\n",
    "from transformers import DefaultDataCollator\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from shutil import rmtree\n",
    "from os.path import split\n",
    "\n",
    "\n",
    "def pad_pooler_output(pooler_output):\n",
    "    padded_pooler_output = pad_sequences(pooler_output,\n",
    "                                         padding=\"post\",\n",
    "                                         dtype=\"float32\")\n",
    "    return {\"padded_pooler_output\": padded_pooler_output}\n",
    "\n",
    "\n",
    "def balance_dataset(hf_dataset):\n",
    "    conversation_series = Series(hf_dataset[\"conversation_label\"])\n",
    "    normal_conversation_series = conversation_series[conversation_series ==\n",
    "                                                     False]\n",
    "    perverted_conversation_series = conversation_series[conversation_series ==\n",
    "                                                        True]\n",
    "    normal_conversation_series = normal_conversation_series.sample(\n",
    "        frac=len(perverted_conversation_series) /\n",
    "        len(normal_conversation_series),\n",
    "        random_state=CONSTANTS.PREPROCESSING.RANDOMNESS.SEED)\n",
    "\n",
    "    hf_dataset = hf_dataset.select(\n",
    "        perverted_conversation_series.index.to_list() +\n",
    "        normal_conversation_series.index.to_list())\n",
    "\n",
    "    return hf_dataset\n",
    "\n",
    "\n",
    "hf_dataset = DatasetDict([\n",
    "    (\"train\",\n",
    "     load_from_disk(\n",
    "         CONSTANTS.DATASETS[CONSTANTS.INPUTS.FOCUS_MODEL].TRAINING)),\n",
    "    (\"test\",\n",
    "     load_from_disk(CONSTANTS.DATASETS[CONSTANTS.INPUTS.FOCUS_MODEL].TESTING))\n",
    "])\n",
    "hf_dataset[\"train\"] = balance_dataset(hf_dataset[\"train\"])\n",
    "hf_dataset[\"test\"] = balance_dataset(hf_dataset[\"test\"])\n",
    "\n",
    "hf_dataset[\"train\"] = hf_dataset[\"train\"].train_test_split(\n",
    "    train_size=CONSTANTS.PREPROCESSING.DATASET_SPLITS.TRAINING,\n",
    "    shuffle=True,\n",
    "    seed=CONSTANTS.PREPROCESSING.RANDOMNESS.SEED)\n",
    "\n",
    "hf_training_dataset = hf_dataset[\"train\"][\"train\"]\n",
    "hf_validation_dataset = hf_dataset[\"train\"][\"test\"]\n",
    "hf_testing_dataset = hf_dataset[\"test\"]\n",
    "\n",
    "hf_hyperdataset = hf_training_dataset.train_test_split(\n",
    "    train_size=CONSTANTS.PREPROCESSING.HYPERDATASET_FRACTION,\n",
    "    shuffle=True,\n",
    "    seed=CONSTANTS.PREPROCESSING.RANDOMNESS.SEED)[\"train\"]\n",
    "hf_hyperdataset = hf_hyperdataset.train_test_split(\n",
    "    train_size=CONSTANTS.PREPROCESSING.HYPERDATASET_SPLITS.TRAINING,\n",
    "    shuffle=True,\n",
    "    seed=CONSTANTS.PREPROCESSING.RANDOMNESS.SEED)\n",
    "hf_hypertraining_dataset = hf_hyperdataset[\"train\"]\n",
    "hf_hypervalidation_dataset = hf_hyperdataset[\"test\"]\n",
    "\n",
    "hf_training_dataset = hf_training_dataset.map(\n",
    "    lambda example: pad_pooler_output(example[\"pooler_output\"]),\n",
    "    batched=True,\n",
    "    batch_size=CONSTANTS.MODELS.BERT.BATCHING.SIZE)\n",
    "hf_validation_dataset = hf_validation_dataset.map(\n",
    "    lambda example: pad_pooler_output(example[\"pooler_output\"]),\n",
    "    batched=True,\n",
    "    batch_size=CONSTANTS.MODELS.BERT.BATCHING.SIZE)\n",
    "hf_testing_dataset = hf_testing_dataset.map(\n",
    "    lambda example: pad_pooler_output(example[\"pooler_output\"]),\n",
    "    batched=True,\n",
    "    batch_size=CONSTANTS.MODELS.BERT.BATCHING.SIZE)\n",
    "hf_hypertraining_dataset = hf_hypertraining_dataset.map(\n",
    "    lambda example: pad_pooler_output(example[\"pooler_output\"]),\n",
    "    batched=True,\n",
    "    batch_size=CONSTANTS.MODELS.BERT.BATCHING.SIZE)\n",
    "hf_hypervalidation_dataset = hf_hypervalidation_dataset.map(\n",
    "    lambda example: pad_pooler_output(example[\"pooler_output\"]),\n",
    "    batched=True,\n",
    "    batch_size=CONSTANTS.MODELS.BERT.BATCHING.SIZE)\n",
    "\n",
    "default_data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "to_tf_dataset_kwargs = {\n",
    "    \"columns\": [\"padded_pooler_output\"],\n",
    "    \"label_cols\": [\"conversation_label\"],\n",
    "    \"batch_size\": CONSTANTS.MODELS.BERT.BATCHING.SIZE,\n",
    "    \"collate_fn\": default_data_collator,\n",
    "    \"shuffle\": False\n",
    "}\n",
    "\n",
    "tf_training_dataset = hf_training_dataset.to_tf_dataset(**to_tf_dataset_kwargs)\n",
    "tf_validation_dataset = hf_validation_dataset.to_tf_dataset(\n",
    "    **to_tf_dataset_kwargs)\n",
    "tf_testing_dataset = hf_testing_dataset.to_tf_dataset(**to_tf_dataset_kwargs)\n",
    "tf_hypertraining_dataset = hf_hypertraining_dataset.to_tf_dataset(\n",
    "    **to_tf_dataset_kwargs)\n",
    "tf_hypervalidation_dataset = hf_hypervalidation_dataset.to_tf_dataset(\n",
    "    **to_tf_dataset_kwargs)\n",
    "\n",
    "rmtree(split(CONSTANTS.DATASETS[CONSTANTS.INPUTS.FOCUS_MODEL].TRAINING)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar modelos dinámicamente en función de los hiperparámetros\n",
    "\n",
    "El hiperafinador de Keras Tuner requiere que se defina una función que reciba a un objeto de hiperparámetros y se retorne un modelo en función de lo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Dropout, Masking, LSTM\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras import Model, Sequential\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "    loss = BinaryCrossentropy(from_logits=True)\n",
    "    metrics = BinaryAccuracy()\n",
    "    optimizer = SGD(learning_rate=hp.Float(\n",
    "        \"learning_rate\",\n",
    "        min_value=CONSTANTS.HYPERTUNING.OPTIMIZERS.SGD.LEARNING_RATE.MIN_VALUE,\n",
    "        max_value=CONSTANTS.HYPERTUNING.OPTIMIZERS.SGD.LEARNING_RATE.MAX_VALUE,\n",
    "        sampling=CONSTANTS.HYPERTUNING.OPTIMIZERS.SGD.LEARNING_RATE.SAMPLING))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Input(shape=(None, CONSTANTS.MODELS.BERT.POOLER_OUTPUT_LENGTH),\n",
    "              dtype=\"float32\"))\n",
    "    model.add(Masking(mask_value=0, dtype=\"float32\"))\n",
    "    model.add(\n",
    "        LSTM(units=hp.Int(\n",
    "            \"lstm_units\",\n",
    "            min_value=CONSTANTS.HYPERTUNING.LAYERS.LSTM.UNITS.MIN_VALUE,\n",
    "            max_value=CONSTANTS.HYPERTUNING.LAYERS.LSTM.UNITS.MAX_VALUE,\n",
    "            step=CONSTANTS.HYPERTUNING.LAYERS.LSTM.UNITS.STEP)))\n",
    "    model.add(\n",
    "        Dense(units=hp.Int(\n",
    "            \"dense_units\",\n",
    "            min_value=CONSTANTS.HYPERTUNING.LAYERS.DENSE.UNITS.MIN_VALUE,\n",
    "            max_value=CONSTANTS.HYPERTUNING.LAYERS.DENSE.UNITS.MAX_VALUE,\n",
    "            step=CONSTANTS.HYPERTUNING.LAYERS.DENSE.UNITS.STEP),\n",
    "              activation=\"relu\"))\n",
    "    model.add(\n",
    "        Dropout(rate=hp.Float(\n",
    "            \"dropout_rate\",\n",
    "            min_value=CONSTANTS.HYPERTUNING.LAYERS.DROPOUT.RATE.MIN_VALUE,\n",
    "            max_value=CONSTANTS.HYPERTUNING.LAYERS.DROPOUT.RATE.MAX_VALUE,\n",
    "            step=CONSTANTS.HYPERTUNING.LAYERS.DROPOUT.RATE.STEP)))\n",
    "    model.add(Dense(1, activation=None, name=\"classifier\"))\n",
    "    model.compile(optimizer=optimizer, metrics=metrics, loss=loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de memoria a nivel de disco\n",
    "\n",
    "De acuerdo a las [especificaciones técnicas](https://www.kaggle.com/docs/notebooks#technical-specifications) de los *notebooks* en Kaggle, la carpeta `/kaggle/working` tiene una capacidad máxima de 20 \\[GB\\]. Por ello, crearemos la carpeta temporal `/kaggle/tmp` que presentará dos ventajas:\n",
    "\n",
    "1. Tendrá una capacidad en disco cercana al triple de lo que hay en `/kaggle/working`.\n",
    "1. Cualquier archivo generado en `/kaggle/tmp` será borrado una vez se cierre la sesión.\n",
    "\n",
    "Solo se utilizará para guardar los *trials* de la tapa de hiperafinación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "\n",
    "makedirs(CONSTANTS.HYPERTUNING.PATHS.ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa de hiperafinación\n",
    "\n",
    "Realizaremos la búsqueda de los hiperparámetros a través del método de búsqueda aleatoria pues permite ahondar mucho más el espacio de búsqueda. Se construirá un modelo a partir de los mejores hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from shutil import rmtree\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel=model_builder,\n",
    "    objective=CONSTANTS.HYPERTUNING.ALGORITHMS.RANDOM_SEARCH.OBJECTIVE,\n",
    "    max_trials=CONSTANTS.HYPERTUNING.ALGORITHMS.RANDOM_SEARCH.MAX_TRIALS,\n",
    "    seed=CONSTANTS.PREPROCESSING.RANDOMNESS.SEED,\n",
    "    directory=CONSTANTS.HYPERTUNING.PATHS.ROOT,\n",
    "    project_name=CONSTANTS.HYPERTUNING.PATHS.RANDOM_SEARCH.SGD)\n",
    "\n",
    "tuner.search(\n",
    "    tf_hypertraining_dataset,\n",
    "    validation_data=tf_hypervalidation_dataset,\n",
    "    epochs=CONSTANTS.HYPERTUNING.ALGORITHMS.RANDOM_SEARCH.EPOCHS_PER_TRIAL)\n",
    "\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "model = tuner.hypermodel.build(best_hyperparameters)\n",
    "\n",
    "print(\"\\nhyperparameters:\", best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa de entrenamiento\n",
    "\n",
    "Se definirá un único *callback* llamado `ModelCheckpoint` que permitirá guardar los modelos obtenidos en cada *epoch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "history = model.fit(\n",
    "    tf_training_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            CONSTANTS.TRAINING.CALLBACKS.MODEL_CHECKPOINT.FILEPATH[\n",
    "                CONSTANTS.INPUTS.FOCUS_MODEL],\n",
    "            monitor=CONSTANTS.TRAINING.CALLBACKS.MODEL_CHECKPOINT.MONITOR,\n",
    "            save_best_only=False,\n",
    "            save_weights_only=False)\n",
    "    ],\n",
    "    epochs=CONSTANTS.TRAINING.EPOCHS,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de métricas en base al conjunto de pruebas\n",
    "\n",
    "Dado que los conjuntos que trabajamos son instancias de la clase padre `tensorflow.data.Dataset`, no podemos acceder directamente a las etiquetas en un formato *slicing* (o similar), por lo que deberemos recorrer *batch* por *batch* y concatenar los resultados en un vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, concatenate\n",
    "\n",
    "\n",
    "def get_tf_dataset_labels(tf_dataset):\n",
    "    concatenated_labels = array([], dtype=\"int32\")\n",
    "\n",
    "    for _, labels in tf_dataset:\n",
    "        concatenated_labels = concatenate((concatenated_labels, labels))\n",
    "\n",
    "    return concatenated_labels\n",
    "\n",
    "true_labels = get_tf_dataset_labels(tf_testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para medir la capacidad de generalización del modelo recién entrenado, definiremos la función `mesaure_model_metrics`, la cual retornará cinco valores (en orden):\n",
    "\n",
    "1. Exactitud o *binary accuracy*.\n",
    "1. Pérdida o *binary crossentropy*.\n",
    "1. Recuperación o *recall*.\n",
    "1. Precisión o *precision*.\n",
    "1. Valor-F o *F-score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "\n",
    "def measure_model_metrics(true_labels, predicted_labels):\n",
    "    binary_accuracy = BinaryAccuracy()\n",
    "    binary_accuracy.update_state(true_labels, predicted_labels)\n",
    "\n",
    "    binary_crossentropy = BinaryCrossentropy(from_logits=False)(\n",
    "        true_labels, predicted_labels)\n",
    "\n",
    "    recall = Recall()\n",
    "    recall.update_state(true_labels, predicted_labels)\n",
    "\n",
    "    precision = Precision()\n",
    "    precision.update_state(true_labels, predicted_labels)\n",
    "\n",
    "    f_score = 2 * (precision.result().numpy() * recall.result().numpy()) / (\n",
    "        precision.result().numpy() + recall.result().numpy())\n",
    "\n",
    "    return binary_accuracy.result().numpy(), binary_crossentropy.numpy(\n",
    "    ), recall.result().numpy(), precision.result().numpy(), f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, obtendremos las métricas de desempeño del modelo durante cada *epoch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.saving import load_model\n",
    "from numpy import array\n",
    "from tensorflow import sigmoid\n",
    "from sklearn.metrics import confusion_matrix as measure_confusion_matrix\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "for epoch in range(CONSTANTS.TRAINING.EPOCHS):\n",
    "    model = load_model(CONSTANTS.TRAINING.CALLBACKS.MODEL_CHECKPOINT.FILEPATH[\n",
    "        CONSTANTS.INPUTS.FOCUS_MODEL].format(epoch=epoch + 1))\n",
    "    predicted_labels = sigmoid(model.predict(tf_testing_dataset,\n",
    "                                             verbose=1)).numpy().flatten()\n",
    "    accuracy, loss, recall, precision, f_score = measure_model_metrics(\n",
    "        true_labels, predicted_labels)\n",
    "\n",
    "    confusion_matrix = measure_confusion_matrix(true_labels,\n",
    "                                                predicted_labels > THRESHOLD)\n",
    "\n",
    "    print(\"\\n========\\nepoch {epoch:02d}\\n========\\n\".format(epoch=epoch + 1))\n",
    "    print(\"metrics\\n=======\\n\")\n",
    "    print(f\"- accuracy: {accuracy}\")\n",
    "    print(f\"- loss: {loss}\")\n",
    "    print(f\"- recall: {recall}\")\n",
    "    print(f\"- precision: {precision}\")\n",
    "    print(f\"- f-score: {f_score}\")\n",
    "    print(\"\\nconfusion matrix\\n================\\n\")\n",
    "    print(confusion_matrix, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar resultados\n",
    "\n",
    "Con el afán de poder recrear los resultados, se persisten los conjuntos de datos generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_training_dataset.save_to_disk(\n",
    "    f\"outputs/phase_2/datasets/with_atentions/training/huggingface/{CONSTANTS.INPUTS.FOCUS_MODEL}\"\n",
    ")\n",
    "hf_validation_dataset.save_to_disk(\n",
    "    f\"outputs/phase_2/datasets/with_atentions/validation/huggingface/{CONSTANTS.INPUTS.FOCUS_MODEL}\"\n",
    ")\n",
    "hf_validation_dataset.save_to_disk(\n",
    "    f\"outputs/phase_2/datasets/with_atentions/testing/huggingface/{CONSTANTS.INPUTS.FOCUS_MODEL}\"\n",
    ")\n",
    "hf_hypertraining_dataset.save_to_disk(\n",
    "    f\"outputs/phase_2/datasets/with_atentions/hypertraining/huggingface/{CONSTANTS.INPUTS.FOCUS_MODEL}\"\n",
    ")\n",
    "hf_hypervalidation_dataset.save_to_disk(\n",
    "    f\"outputs/phase_2/datasets/with_atentions/hypervalidation/huggingface/{CONSTANTS.INPUTS.FOCUS_MODEL}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
